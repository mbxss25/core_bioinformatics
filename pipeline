#Script to follow in unix
##download dataset from the readme file.

#First, let’s create a directory to save our fastQC outputs to:

mkdir -p QC_Reports 

##run FastQC on our target sample
#Quality check using FastQC

fastqc ERR5743893_1.fastq ERR5743893_2.fastq --outdir QC_Reports 

#once qc is completed, run 
multiqc . 

#then run follwing, to open multi_qc reports of the quality check we performed.

open multiqc_report.html

#multiqc_report.html will give overview of quality of our sequences,I have uploaded report by the name "multiqc_plots.zip", have a look.


##Mapping
#There are a few different tools that can be used to map sequences to a reference but we’re going to use BWA-MEM.

#create a directory to store the results from BWA-MEM:

mkdir Mapping

#indexing

bwa index MN908947.fasta 

#map the sequences from our target sample to the reference genome

bwa mem MN908947.fasta ERR5743893_1.fastq ERR5743893_2.fastq > Mapping/ERR5743893.sam

#Now go to directort /Mapping, ypu will see "ERR5743893.sam". we’re going to convert the SAM file to a BAM file using samtools.

samtools view -@ 20 -S -b Mapping/ERR5743893.sam > Mapping/ERR5743893.bam

#we want a BAM file of reads and mapping information ordered by where in the reference genome those reads were aligned

samtools sort -@ 32 -o Mapping/ERR5743893.sorted.bam Mapping/ERR5743893.bam

#index the sorted BAM file

samtools index Mapping/ERR5743893.sorted.bam

#visualize your BAM file on Integrative Genomics Viewer (IGV)| refer: https://youtu.be/die924mosh0?si=NUi5QWlImCdx-a5E

##Variant calling using FreeBayes
#First we need to index the reference genome using samtools:

samtools faidx MN908947.fasta

#Now, let’s use FreeBayes to identify the variants in our test sample:

freebayes -f MN908947.fasta Mapping/ERR5743893.sorted.bam  > ERR5743893.vcf

#compress and index the VCF file

bgzip ERR5743893.vcf
tabix ERR5743893.vcf.gz

##Setting up Nextflow

#set up your conda channels:

conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

#create an environment called nextflow and install Nextflow in it

conda create --name nextflow nextflow
conda activate nextflow
nextflow help

#Creating a samplesheet for viralrecon
#create a text file called samples.txt in your working directory. Copy and paste the ENA accessions below into this file:

ERR5556343

SRR13500958

ERR5743893

ERR5181310

ERR5405022

#use a for loop to run fastq-dump on each of the accessions in samples.txt:

for i in $(cat samples.txt);do fastq-dump --split-files $i;done

#compress the fastq files

gzip *.fastq

#create a directory called data and move the fastq files there:

mkdir data
mv *.fastq.gz data

##viralrecon requires a CSV file containing the sample names and location of the fastq files input. The format of the samplesheet we’re going to use should look like this:
#sample,fastq_1,fastq_2
#SAMPLE_1,fastq_1_location,fastq_2_location

#python script that can be used to generate the samplesheet automatically.

wget -L https://raw.githubusercontent.com/nf-core/viralrecon/master/bin/fastq_dir_to_samplesheet.py

#Now run the python script:

python3 fastq_dir_to_samplesheet.py data samplesheet.csv -r1 _1.fastq.gz -r2 _2.fastq.gz

#check file 
cat samplesheet.csv

##Running Viralrecon

#activate nextflow
conda activate nextflow

#let’s run viralrecon

nextflow run nf-core/viralrecon -profile conda \
--max_memory '12.GB' --max_cpus 4 \
--input samplesheet.csv \
--outdir results/viralrecon \
--protocol amplicon \
--genome 'MN908947.3' \
--primer_set artic \
--primer_set_version 3 \
--skip_kraken2 \
--skip_assembly \
--skip_pangolin \
--skip_nextclade \
--skip_asciigenome \
--platform illumina \
-resume

##View results

#nav result directory
cd results/viralrecon
ls

#read multiqc
open Multiqc

#refer this file in branches to see my reports
multiqc_report.html

#Tidying up after Nextflow
#check usage of workspace

du -sh work

#remove if needed

##Vizualization 
#insatall all necessary packages
#couple of code to undrstand data

list.files()
var <- read.csv("variants_long_table.csv")
head(var)
dim(var)
nrow(var)
ncol(var)
str(var)
# Summary statistics of the whole data or specified columns

## For the whole table 

> summary(var)

## For non-numerical data

> summary(var$SAMPLE)

Length     Class      Mode 
153 character character 

## For numerical data

> summary(var$DP)

Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
38     369    1014    2635    2766   41836 

# Check the class of your data

> class(var)

[1] "data.frame"

# Check the class of an object

> class(var$CHROM)

[1] "character"

> typeof(var$CHROM)

[1] "character"

# Preview the data using a spreadsheet-style data viewer in RStudio
> View(var)

#Filtering columns

#check column name
colnames(var)

#select column 1,4 and 4

var[,c(1,4,5)]

# Select columns 1, 4, and 5 with default display

select(var, SAMPLE, REF, ALT)

# Select columns 1, 4, and 5 with selected display
select(var, SAMPLE, REF, ALT) %>% head(3)

# Select all columns except the column “CALLER” with selected display
select(var, -CALLER) %>% head(3)

## Transform the data frame into a tibble

var_tb <- as_tibble(var)
select(var_tb, SAMPLE, REF, ALT) %>% head(3)

#filtering row

#Select rows with selected display using base R code
> var_tb[var_tb$SAMPLE == "SRR13500958",]

# Select rows with selected display using dplyr functions
> filter(var_tb, SAMPLE == "SRR13500958") %>% head(3)

#Filtering columns and rows

# Select sample type (rows) and variables (columns) with selected display

> var_tb %>% filter(SAMPLE == "SRR13500958") %>% select(CHROM, POS, REF, ALT) %>% head(3)

# To select all data related to the sample specified

> var_tb %>% filter(SAMPLE == "SRR13500958") %>% select(CHROM, POS, REF, ALT, DP)

# To select only values for which DP>=500 for the same sample

> var_tb %>% filter(SAMPLE == "SRR13500958" & DP>=500) %>% select(CHROM, POS, REF, ALT, DP)

# To select only values for which DP>=1000 for the same sample

> var_tb %>% filter(SAMPLE == "SRR13500958" & DP>=1000) %>% select(CHROM, POS, REF, ALT, DP)
